---
title: "Homework 2"
author: "Chike Odenigbo"
date: "November 6, 2022"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
#install.packages("knitr")
#knitr::opts_chunk$set(echo = TRUE)
#install.packages("ISLR")
#install.packages('rpart')
#install.packages("ipred")
#install.packages("randomForest")
#install.packages("adabag")
#install.packages("rpart.plot")
#install.packages("caret")
#install.packages('mogavs')
#install.packages("factoextra")
#install.packages('pls')
#install.packages('psy')
#install.packages("fossil")
library(fossil)
library(psy)
library(pls)
library(mogavs)
library(factoextra)
library(randomForest)
library(ipred)
library(ISLR)
library(rpart)
library(adabag)
library(rpart.plot)
library(caret)
set.seed(123)
```

```{r}
get_accuracy <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  accuracy = round((TP + TN) / sum(TP,FP,TN,FN), 2)
  return(accuracy)
}

get_classification_error_rate <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  classification_error_rate = round((FP + FN) / sum(TP,FP,TN,FN),2)
  return(classification_error_rate)
}

get_precision <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  precision = round(TP / (TP + FP), 2)
  return(precision)
}


get_sensitivity <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  sensitivity = round(TP / (TP + FN), 2)
  return(sensitivity)
}

get_specificity <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  specificity = round(TN / (TN + FP), 2)
  return(specificity)
}

get_f1_score <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  
  precision = round(TP / (TP + FP), 2)
  sensitivity = round(TP / (TP + FN), 2)
  f1_score = round((2 * precision * sensitivity) / (precision + sensitivity), 2)
  return(f1_score)
}

get_false_positive_rate <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  
  fpr = round(FP / (FP + TN), 2)
  return(fpr)
}

get_false_negative_rate <- function(predicted, actual){
  confusion_table = table(predicted, actual)
  TP = confusion_table[2,2]
  TN = confusion_table[1,1]
  FN = confusion_table[1,2]
  FP = confusion_table[2,1]
  
  fnr = round(FN / (FN + TP), 2)
  return(fnr)
}
```
## Section 1
### Question 1.1
Create a training set of 800 observations, and a test set containing the rest using the following code:
```{r}
# ```{r pressure, echo=FALSE}
n <- nrow(OJ)
set.seed(1234)
id.train=sample(1:n,size=800)
id.test=setdiff(1:n,id.train)
OJ$Purchase = as.factor(OJ$Purchase)
OJ$Store7 = as.factor(OJ$Store7)
as.data.frame(sapply(OJ,class))
OJ.train=OJ[id.train,-3]
OJ.test=OJ[id.test,-3]
```

### Question 1.2
Construct an unpruned classification tree to predict the variable purchase using the available predictors. Calculate the false positive rate, false negative rate and overall error rate of this tree on the test data (note: you can use your code from the previous assignment directly for this question).
```{r}
# ```{r pressure, echo=FALSE}
tree <- rpart(OJ.train$Purchase ~ . ,data = OJ.train, method = 'class')
#summary(tree)

pred = predict(tree,newdata = OJ.test,type = c("class"))

print(paste0("False Positive Rate: ", get_false_positive_rate(OJ.test$Purchase,pred)))

print(paste0("Error Rate: ", get_classification_error_rate(OJ.test$Purchase,pred)))

print(paste0("False Negative Rate: ", get_false_negative_rate(OJ.test$Purchase,pred)))

```


### Question 1.3
Using all the data (training and test put together), use the bagging approach to do the same analysis again and compare the results of the different error rates.
```{r}
set.seed(1)
n_pred <- ncol(OJ.train) - 1

bag.OJ <- randomForest(OJ.train$Purchase ~ .,
                           data = OJ.train,
                           mtry=n_pred, 
                           importance=TRUE)

bag.pred = predict(bag.OJ,newdata = OJ.test,type = c("class"))

print(paste0("False Positive Rate: ", get_false_positive_rate(OJ.test$Purchase,bag.pred)))

print(paste0("Error Rate: ", get_classification_error_rate(OJ.test$Purchase,bag.pred)))

print(paste0("False Negative Rate: ", get_false_negative_rate(OJ.test$Purchase,bag.pred)))

```

### Question 1.4
Using all the data (training and test put together), use the random forest approach to redo the same analysis and compare the results of the different error rates.
```{r}
set.seed(1)

rf.OJ <- randomForest(OJ.train$Purchase ~ .,
                           data = OJ.train,
                           #mtry=n_pred, 
                           importance=TRUE)

rf.pred = predict(rf.OJ,newdata = OJ.test,type = c("class"))

print(paste0("False Positive Rate: ", get_false_positive_rate(OJ.test$Purchase,rf.pred)))

print(paste0("Error Rate: ", get_classification_error_rate(OJ.test$Purchase,rf.pred)))

print(paste0("False Negative Rate: ", get_false_negative_rate(OJ.test$Purchase,rf.pred)))

```

### Question 1.5
Calculate the importance of the variables in the classification tree constructed in 2) and the forest constructed in 4). Compare.
```{r}
set.seed(1)

as.data.frame(importance(rf.OJ))
varImpPlot(rf.OJ)
```

```{r}
set.seed(1)

as.data.frame(tree$variable.importance)
#varImpPlot(tree)
```

### Question 1.6
Using all the data (training and test put together), use the boosting approach to do the same analysis again and compare the results of the error rates between all tested methods.
```{r}
set.seed(1)

boost.OJ = boosting(Purchase ~ ., 
                    data=OJ.train, 
                    #mfinal = 100, 
                    coeflearn = 'Freund')#,
                    #control=rpart.control(maxdepth=10))
boost.pred = predict(boost.OJ,newdata = OJ.test,type = c("class"))

print(paste0("False Positive Rate: ", get_false_positive_rate(OJ.test$Purchase,boost.pred$class)))

print(paste0("Error Rate: ", get_classification_error_rate(OJ.test$Purchase,boost.pred$class)))

print(paste0("False Negative Rate: ", get_false_negative_rate(OJ.test$Purchase,boost.pred$class)))
```

## Section 2
### Question 2.1
Create a training and test dataset of size 300 and 100 respectively using the following code:
```{r}
set.seed(123456)
n=nrow(Carseats)
id.train=sample(1:n,size=300)
id.test=setdiff(1:n,id.train)
Carseat.train=Carseats[id.train,]
Carseat.test=Carseats[id.test,]
```

### Question 2.2
Construct a regression tree predicting the variable Sales from the other variables available in the training sample. Graph the tree and interpret the results.

**ANSWER:**
*** ***
```{r}
tree.reg <- rpart(Sales ~ . ,data = Carseat.train, method = 'anova')
```

```{r}
rpart.plot(tree.reg)
```

### Question 2.3
Calculate the MSE on the test data.
```{r}
pred.reg.tree = predict(tree.reg, newdata = Carseat.test) 
mean((pred.reg.tree - Carseat.test$Sales)^2)
```
### Question 2.4
Use cross-validation to determine the optimal level of complexity parameter to prune the tree. Does pruning improve the MSE?
```{r}
set.seed(400)
data_ctrl = trainControl(method = "cv", number = 10)
mytree.caret = train(Sales~.,data=Carseat.train, method = "rpart",  trControl=data_ctrl)
#mytree.caret$resample
#mytree.caret$results
mytree.caret$bestTune
pred.cv.tree = predict(mytree.caret,newdata = Carseat.test)
mean((pred.cv.tree - Carseat.test$Sales)^2)
```

### Question 2.5
Repeat the previous question several times (with a different seed) - and look at the differences between the results obtained. What do you conclude?
```{r}
#set.seed(0)
seed.list = sample.int(100, 10)
df = data.frame()

for (seed in seed.list){
  set.seed(seed)
  data_ctrl = trainControl(method = "cv", number = 10)
  mytree.caret = train(Sales~.,data=Carseat.train, method = "rpart",  trControl=data_ctrl)
  #mytree.caret$resample
  #mytree.caret$results
  mytree.caret$bestTune
  pred.cv.tree = predict(mytree.caret,newdata = Carseat.test)
  mse = mean((pred.cv.tree - Carseat.test$Sales)^2)
  output = c(seed,mse)
  df = rbind(df, output)
}
colnames(df)<-c("seed", "mse")
df
```
### Question 2.6
Repeat the previous question several times (with a different seed) - and look at the differences between the results obtained. What do you conclude?
```{r}
set.seed(1)
n_pred <- ncol(Carseat.train) - 1

bag.reg <- randomForest(Carseat.train$Sales ~ .,
                           data = Carseat.train,
                           mtry=n_pred, 
                           importance=TRUE)

bag.reg.pred = predict(bag.reg,newdata = Carseat.test)#,type = c("anova"))
mean((bag.reg.pred - Carseat.test$Sales)^2)
```
### Question 2.7
Use the ***importance*** function to evaluate the importance of variables. Compare with the tree constructed at the beginning of the exercise.
```{r}
as.data.frame(importance(bag.reg))
varImpPlot(bag.reg)
```
### Question 2.8
Repeat the 2 previous questions using the random forest algorithm. Comment.**
```{r}
set.seed(1)
n_pred <- ncol(Carseat.train) - 1

rf.reg <- randomForest(Carseat.train$Sales ~ .,
                           data = Carseat.train,
                           #mtry=n_pred, 
                           importance=TRUE)

rf.reg.pred = predict(rf.reg,newdata = Carseat.test)#,type = c("anova"))
mean((rf.reg.pred - Carseat.test$Sales)^2)
```

```{r}
as.data.frame(importance(rf.reg))
varImpPlot(rf.reg)
```

## Section 3
### Question 3.1
Take the data and example from the principal component regression tutorial. Using the graph of the number of PCs versus the cumulative proportion of variance explained, identify a number of PCs at which the variance explained seems to increase only marginally (note: I don’t expect you all to have the same answer). Run a principal component regression with this number of PCs and compare your results with those in the tutorial (MSE of the validation set).

**ANSWER:**
***I would save the first 50 components***


```{r}
set.seed(60603)
id=sample(1:nrow(crimeData))
crimeData.pca = crimeData[id[1:1450],][,-123]
crimeData.train=data.frame(apply(crimeData[id[1:1450],],2,scale))
crimeData.valid=data.frame(apply(crimeData[id[1451:nrow(crimeData)],],2,scale))
pca.res = prcomp(crimeData.pca, center = TRUE, scale. = TRUE)
#summary(pca.res)$importance
#round(pca.res$rotation,2)
#data.frame(pca.res$x)

plot(summary(pca.res)$importance[3,], xlab = "Number of Principal Components", ylab = "Proportion of Variance Explained" )

model.crimeData.pca = pcr(y~., data = crimeData.train)


pred.valid.tutorial=predict(model.crimeData.pca,newdata=crimeData.valid,ncomp=15)
pred.valid.answer=predict(model.crimeData.pca,newdata=crimeData.valid,ncomp=50)
MSE.valid.tutorial = sqrt(mean((crimeData.valid$y-pred.valid.tutorial)^2))
MSE.valid.tutorial

MSE.valid.answer = sqrt(mean((crimeData.valid$y-pred.valid.answer)^2))
MSE.valid.answer

rbind(MSE.valid.tutorial,MSE.valid.answer)
```

### Question 3.2
It is also possible to choose the number of optimal PCs by using other criteria than the explained variance, such as the MSEP (mean squared error of prediction) or the R2 calculated by cross validation. You can obtain the graph of the number of PCs versus one of these measures with the following command (for the MSEP):

**ANSWER:**
***I would save the first 50 components***


```{r}
set.seed(60603)
id=sample(1:nrow(crimeData))
crimeData.pca = crimeData[id[1:1450],][,-123]
crimeData.train=data.frame(apply(crimeData[id[1:1450],],2,scale))
crimeData.valid=data.frame(apply(crimeData[id[1451:nrow(crimeData)],],2,scale))
pca.res = prcomp(crimeData.pca, center = TRUE, scale. = TRUE)

model.pcr.rmsep=pcr(y~., data = crimeData.train, validation="CV")
validationplot(model.pcr.rmsep, val.type="RMSEP", cex.axis=0.7)
axis(side = 1, at = c(8), cex.axis=0.7)
abline(v = 8, col = "blue", lty = 3)
```


### Question 3.3
Repeat the previous question using the R2, with the option **val.type="R2"**
**ANSWER:**
***I would save the first 50 components***


```{r}
set.seed(60603)
id=sample(1:nrow(crimeData))
crimeData.pca = crimeData[id[1:1450],][,-123]
crimeData.train=data.frame(apply(crimeData[id[1:1450],],2,scale))
crimeData.valid=data.frame(apply(crimeData[id[1451:nrow(crimeData)],],2,scale))
pca.res = prcomp(crimeData.pca, center = TRUE, scale. = TRUE)

model.pcr.r2=pcr(y~., data = crimeData.train, validation="CV")
validationplot(object = model.pcr.r2, "validation",val.type = "R2")#, cex.axis=0.7)
axis(side = 1, at = c(8), cex.axis=0.7)
abline(v = 8, col = "blue", lty = 3)
```

## Section 4 Factor Analysis
### Question 4.1
In this question, we will analyze data on different measures of pastry texture. The data have 50 observations and 5 variables were measured:

* Oil: oil content of the pastry
* Density: density of the product (the higher the number, the denser the product)
* Crispy: a measure of “crispness” on a scale of 7 to 15, with 15 representing the crispiest pastry
* Fracture: the angle, in degrees, at which the pastry can be bent before it breaks
* Hardness: force required to break the pastry

The data are available in the file food.csv. The column labeled X represents the participant ID.

Perform a factor analysis on these data. Identify the optimal number of factors, describe them and interpret them.
**ANSWER:**
***I would use 2 factors for the food data set given that the p-value for 2 factors of 0.603 is significantly greater than the pvalue of 1.82*10^-7. Specifying only 1 factor seems to try to force the oil as a contributor but it does not have any real semantic meaning to the other variables yet the algorithm determines it has a strong impact on the factor. The first factor will be composed of density (0.9 weight in defining factor) and fracture (0.64 correlation weight in defining factor) and the second factor will be composed of crispy (0.64 weight in defining factor) and hardness (0.76 weight in defining factor) given their higher respective loading coefficients to the specific factor. Oil seems to not be assign to any of the factors. The latent variable that factor 1 seems to represent is how hollow the inside of the pastry is which is reflected by the ability of the pastry to bend and its thickness. The latent variable for factor 2 seems to be how soft the outside of the pastry is reflected by its ease of biting and its crispyness. ***
```{r}
food.df = read.table("./Data/food.csv", header=TRUE, sep =",")

Pval.factanal <- function(x)
{
    p <- nrow(x$loadings); factors <- x$factors
    if(!is.na(x$n.obs) && x$dof > 0) {
        dof <- x$dof
        stat <- (x$n.obs - 1 - (2 * p + 5)/6 -
                 (2 * factors)/3) * x$criteria["objective"]
        pchisq(stat, dof, lower.tail = FALSE)
    } else NA
}
factanal.2fact = factanal(food.df[,-1], factors = 2,rotation = "varimax")
factanal.1fact = factanal(food.df[,-1], factors = 1,rotation = "varimax")
paste('the pvalue for 2 factors is: ', Pval.factanal(factanal.2fact))
paste('the pvalue for 1 factor is: ', Pval.factanal(factanal.1fact))

print('2 Factors Results')
(factanal.2fact$loadings)
print('1 Factor1 Results')
(factanal.1fact$loadings)
#Pval.factanal(factanal(food.df[,-1], factors = 1,rotation = "varimax"))
#factanal(food.df[,-1], factors = 3,rotation = "varimax")
#factanal(food.df[,-1], factors = 2,rotation = "varimax")#$scores
#factanal(food.df[,-1], factors = 1,rotation = "varimax")

cronbach(food.df[,c('Density','Fracture')])
cronbach(food.df[,c('Crispy','Hardness')])
cronbach(food.df[,c('Crispy','Hardness','Density','Fracture')])

cronbach(food.df[,-1])

```


## Section 5 K-Means
In this question, you will perform a K-means classification manually with K=2 in the small data example below which contains n=6 observations and p=2 variables:
```{r}
mydata=matrix(c(1,4,1,3,0,4,5,1,6,2,4,0),ncol=2,byrow=T)
colnames(mydata)=c("X1","X2")
row.names(mydata)=1:6
mydata

```

### Question 5.1
Graph the data
```{r}
plot(mydata[,"X1"], mydata[,"X2"])

```

### Question 5.2
At each step of the algorithm, explain what you are doing and i) report the group assignment table to the observations and ii) graph the data by indicating group 1 in red, group 2 in green and adding the 2 centroids on the graph.
**ANSWER:**
**The methodology for the clustering analysis was to compare kmeans clustering with random initialization and hierarchical clustering initialization. Given the underlying structure of the data which has natural clusters, the difference in clusters does does not change much with the two initialization method. The only real change is that the cluster group name changes with random initialization compared to the other 2 methods tested whereby cluster 1 is assigned to the same observations as cluster 2 and vice versa.**
```{r}
mydata=matrix(c(1,4,1,3,0,4,5,1,6,2,4,0),ncol=2,byrow=T)
colnames(mydata)=c("X1","X2")
row.names(mydata)=1:6
# Step 1: Even though variables are on the similarly valued, standardizing is done to ensure they are on same scale
mydata.scaled = scale(mydata,center=TRUE, scale=TRUE)
mydata.scaled
# Step 2: Get distance matrix
distance.scaled = dist(mydata.scaled,method = "euclidean")
distance.scaled
# Step 3: perform hierachical clustering to get initial centroids for kmeans and select 2 clusters which was specified for kmeans
h.clust = hclust(distance.scaled, method = "centroid")
h.clust
h.clust$merge
groups=cutree(h.clust, k=2)
groups
mydata.scaled = (as.data.frame(mydata.scaled))
mydata.scaled$clusters_dendo = groups
mydata = (as.data.frame(mydata))
mydata$clusters_dendo = groups
colors_ = c("red","green")
names(colors_) = c("1","2")
#plot(mydata$X1, mydata$X2 , col = c("red", "green")[mydata$clusters_dendo], main = "Hierarchical Clustering",pch = 19)
plot(mydata$X1, mydata$X2 , col = colors_[mydata$clusters_dendo], main = "Hierarchical Clustering",pch = 19)

# Step 4: Get average per dendorgram cluster to initialize kmeans
kmeans.initial.clusters = subset(aggregate(mydata.scaled[, 1:3], list(mydata.scaled$clusters_dendo), mean),select = c('X1','X2'))

# Step 5: Initialize kmeans object with hclust results
kmeans.hclust = kmeans(x=subset(mydata.scaled,select = c('X1','X2')),centers = kmeans.initial.clusters)

print('Cluster Centers for Kmeans with Hierarchical Clusters Initialization')
kmeans.hclust$centers
print('Cluster Centers from Hierarchical Clusters')
as.matrix(kmeans.initial.clusters)

# Step 6: Kmeans with random initialization
kmeans.rand = kmeans(x=subset(mydata.scaled,select = c('X1','X2')),centers = 2) #using scaled data
#kmeans.rand = kmeans(x=subset(mydata,select = c('X1','X2')),centers = 2)
print('Cluster Centers for Kmeans with Random Initialization')
kmeans.rand$centers
mydata$clusters_km_rand = kmeans.rand$cluster
mydata$clusters_km_hclust = kmeans.hclust$cluster
#plot(mydata$X1, mydata$X2 , col = c("red", "green")[mydata$clusters_km_rand], main = "KMeans Random Initialization",pch = 19)
plot(mydata$X1, mydata$X2 , col = colors_[mydata$clusters_km_rand], main = "KMeans Random Initialization",pch = 19)
points(kmeans.rand$centers[,'X1'][1], kmeans.rand$centers[,'X1'][2], cex = 2, pch = 6, col ="blue")


#plot(mydata$X1, mydata$X2 , col = c("red","green")[mydata$clusters_km_hclust], main = "KMeans Hierarchical Initialization",pch = 19)
kmeans.rand$centers[,'X1'][1]


plot(mydata$X1, mydata$X2 , col = colors_[mydata$clusters_km_hclust], main = "KMeans Hierarchical Initialization",pch = 19)
points(kmeans.hclust$centers[,'X1'][1], kmeans.hclust$centers[,'X1'][2], cex = 2, pch = 6, col ="blue")

print(mydata)
#mydata.scaled
#mydata.scaled = as.data.frame(mydata.scaled)
#groups$merge
#table(groups)
#typeof(groups)
#adj.rand.index(groups,mydata.scaled$Cluster)
#mydata.scaled = data.frame(mydata.scaled)
#mydata.scaled[, 'h_clusters'] = groups$cluster data.frame(mydata2,Cluster=mydata$Cluster)
#kmeans(mydata,2)



```

```{r}
kmeans.rand$centers 
plot(mydata$X1, mydata$X2 , col = colors_[mydata$clusters_km_rand], main = "KMeans Random Initialization",pch = 19)
points(0.6666667, 3.666667, cex = 2, pch = 25, col ="red") # centroids
points(5, 1, cex = 2, pch = 25, col ="green") # centroid
```
```{r}
kmeans.initial.clusters = subset(aggregate(mydata[, 1:3], list(mydata$clusters_dendo), mean),select = c('X1','X2'))
kmeans.hclust = kmeans(x=subset(mydata,select = c('X1','X2')),centers = kmeans.initial.clusters)
kmeans.hclust$centers 
plot(mydata$X1, mydata$X2 , col = colors_[mydata$clusters_km_hclust], main = "KMeans Random Initialization",pch = 19)
points(0.6666667, 3.666667, cex = 2, pch = 25, col ="red") # centroids
points(5, 1, cex = 2, pch = 25, col ="green") # centroid
```
