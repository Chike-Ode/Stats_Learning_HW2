---
title: "Chapter 7 - Ensemble methods"
output:
  html_document: default
  pdf_document: default
  number_sections: true
---

<style>
p.exercice {
background-color: #DBDBDB;
padding: 10px;
border: 1px solid black;
margin-left: 5px;
border-radius: 5px;
font-style: italic;
}
</style>

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse,verbose=FALSE)
```

<br>
<br>

 
 ## Bagging and random forests
 We will continue here to work with the same data as for the classification trees. The goal of the analysis is to predict, using the covariates present in the file, if a customer will make a purchase following a promotional offer. The data are in the file `dbm_final.csv`. 

```{r}
mydata=read.csv("Data/dbm_final.csv")
mydata$region=as.factor(mydata$region)
mydata$yachat=as.factor(mydata$yachat)
mydata.train=mydata[mydata$train==1,-c(12,13,14)]
mydata.test=mydata[mydata$test==1,-c(12,13,14)]
```


The bagging method and random forests are illustrated here with the help of the function `randomForest` which is in the library of the same name, so you must have it installed beforehand. A large number of options are available with this function, many of which are options relating to the trees that will populate the forest and which we saw in the last chapter. The two options that will be of particular interest to us here are `ntree` (number of trees in the forest) and `mtry`, which represents the number of candidate predictors that are randomly chosen at each split of the tree. If `mtry` takes the value $p$, where $p$ is the total number of predictors in the model, then we are in the case of a simple tree bagging algorithm. If `mtry` is less than $p$ (by default $\sqrt{p}$ for classification trees and $p/3$ for regression trees), then the algorithm implemented is that of random forests. 

Let's first look at the results of a simple bagging (here, 10 predictors are available to predict a customer's purchase). **Note that the response variable must be coded as a factor for a classification (not regression) forest to be performed.**
```{r}
library(randomForest)
set.seed(1234)
rf=randomForest(yachat~.,data=mydata.train,ntree=500,mtry=10)
rf
```

The reported out-of-bag (OOB) error rate is 16.2\%. This means that 16.2\% of the observations are misclassified. The confusion matrix gives the details of these errors (rows: true classes, columns: predicted classes). We can see that the false positive rate is 0.063 and that the false negative rate is 0.53, which is very high.


Let's see now how we can use the obtained forest to predict the observations of the test data:

```{r}
predrf=predict(rf,newdata=mydata.test)
head(predrf)
```

The different error rates on the test data can be calculated as follows:
```{r}
# Error rate
sum(predrf!=mydata.test$yachat)/length(predrf)

# Confusion matrix
M=table(predrf,mydata.test$yachat,dnn=c("Prediction","Observation"))
M

# False positive rate
M[2,1]/(M[2,1]+M[1,1])

# False negative rate
M[1,2]/(M[2,2]+M[1,2])
```

We could also construct the ROC curve and calculate the AUC of this model, as we saw for logistic regression. The code is identical to what we have seen in this chapter and we do not detail it here.


Let us now see if the results obtained differ when the random forest algorithm is used, instead of simple bagging - i.e. randomly selecting a number of candidate variables at each node. The code is identical, we just remove the `mtry=10` option to leave the default value.

```{r}
set.seed(1234)
rf2=randomForest(yachat~.,data=mydata.train,ntree=500)
rf2
```

 
The error rate reported 17\%, is a little higher than the bagging rate which was 16.2\%, but this difference is not really significant and is perhaps obtained by chance. Indeed, if we run the same code a second time, with different bootstrap samples, here is what we get: 
```{r}
rf2=randomForest(yachat~.,data=mydata.train,ntree=500)
rf2
```

This time, the reported error rate is 16.4\%, which is also above the bagging rate. However, let's see what happens with the test sample:
```{r}
predrf=predict(rf,newdata=mydata.test)

# Error rate
sum(predrf!=mydata.test$yachat)/length(predrf)

# Confusion matrix
M=table(predrf,mydata.test$yachat,dnn=c("Prediction","Observation"))
M

# False positive rate
M[2,1]/(M[2,1]+M[1,1])

# False negative rate
M[1,2]/(M[2,2]+M[1,2])
```

We see that the results on the test sample are very similar to the bagging results in our case.

We can also quickly look at the effect of the number of trees on the OOB error, as in the code below:


```{r}
set.seed(123)
n.arbre=seq(1,1000,by=50)
erreur=NULL
for (i in n.arbre)
{
  rf=randomForest(yachat~.,data=mydata.train,ntree=i)
  erreur=c(erreur,sum(rf$err.rate[,1])/rf$ntree)
}
erreur
plot(n.arbre, erreur,type="l")
```

We can see that the error decreases with the number of trees, but that from a certain number of trees, the computational cost is perhaps greater than the gain in error.

 It is finally possible to obtain, as for the trees, importance measures for each variable using the `importance` function. We don't detail the calculation of these measures here because this will be covered in the advanced data mining course, but the code is given here. Note that there is an option in the `randomForest` function that allows you to calculate or not the variable importance measures, and it takes the value `FALSE` by default. So in order to get these measures, we need to rerun the random forest algorithm with the value `TRUE`. The code is as follows:
```{r}
set.seed(12355)
rf3=randomForest(yachat~.,data=mydata.train,ntree=500,importance=TRUE)
importance(rf3,type=1)
```


Of course, we can also use all the functions seen above with regression trees and not classification trees. The code is the same, and here is an example where we want to predict the amount spent in the variable `ymontant`:
```{r}
mydata=read.csv("Data/dbm_final.csv")
mydata$region=as.factor(mydata$region)
mydata.train=mydata[mydata$train==1 & mydata$yachat==1,-c(11,13,14)]
mydata.test=mydata[mydata$test==1& mydata$yachat==1,-c(11,13,14)]

# Random forest
set.seed(3344)
rf4=randomForest(ymontant~.,data=mydata.train,ntree=500)
rf4
1-30.21281/var(mydata.train$ymontant)
```

We see that the error rate is now calculated as a function of the MSE. The calculated $R^2$ is more interpretable than the MSE, and is simply calculated as follows:
$$
R^2=1-\frac{MSE}{Var(Y)}
$$
Here is the code to calculate this rate on the test data:
```{r}
predrf=predict(rf4,newdata=mydata.test)
# MSE
MSE=mean((predrf-mydata.test$ymontant)^2)
MSE
# R2
R2=1-MSE/var(mydata.test$ymontant)
R2
```


## Boosting on trees
We will now see how to run the adaboost algorithm seen in the lecture notes. To do this, we will use the `adabag` library of R. The adaboost algorithm seen in the lecture notes is developed for the classification case, and so we return to that example here. 

```{r}
mydata=read.csv("Data/dbm_final.csv")
mydata$region=as.factor(mydata$region)
mydata$yachat=as.factor(mydata$yachat)
mydata.train=mydata[mydata$train==1,-c(12,13,14)]
mydata.test=mydata[mydata$test==1,-c(12,13,14)]
```

The `boosting` function of the `adabag` library allows to implement the adaboost algorithm for classification trees. The underlying function is the `rpart` function that we saw in the previous course. The code is as follows for a boosting using 100 trees and a maximum depth of 10 for each tree:

```{r}
library(adabag)
# boosting with trees of depth 10
myboost=boosting(yachat~., data=mydata.train, mfinal = 100, 
    coeflearn = 'Freund', control=rpart.control(maxdepth=10))
myboost$importance
```
 
  The option `coeflearn = 'Freund'` gives exactly the same version of the algorithm as described in the lecture notes. We can see that the importance of the variables is very different from the random forest case, which predicted `age` and `sex` as the 2 most important variables. Here is the code to make the prediction and obtain the error rate on the test sample:
```{r}
pred=predict(myboost, newdata=mydata.test)
pred$error
M=pred$confusion

# False positive rate
M[2,1]/(M[2,1]+M[1,1])

# False negative rate
M[1,2]/(M[2,2]+M[1,2])

```
 It can be seen that the overall error rate, as well as the false positive and negative error rates, is slightly higher than for the random forest algorithm in our context. This can be explained by the randomness of the analysis or by the fact that we did not try to optimize the input parameters of the tree algorithm.
